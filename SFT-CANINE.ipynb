{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12536390,"sourceType":"datasetVersion","datasetId":7914307}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# canine_lora_sft_train_kaggle.py\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import CanineTokenizer, CanineModel, get_scheduler\nfrom peft import get_peft_model, LoraConfig, TaskType\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.auto import tqdm\nimport random\nimport os\nimport shutil # Import for zipping the output\n\n# --- Configuration ---\nclass SFTConfig:\n    # --- CHANGE: Update paths for Kaggle ---\n    MODEL_ID = \"google/canine-s\"\n    WORD_LIST_PATH = \"/kaggle/input/hangman/words_250000_train.txt\" # Assumes your dataset is named 'hangman'\n    SAVE_DIR = \"/kaggle/working/lora_experts\" # Save output to the writable directory\n    \n    MAX_LENGTH = 32\n    MASK_CHAR = '_'\n    CHAR_VOCAB = \"abcdefghijklmnopqrstuvwxyz\"\n    char2idx = {c: i for i, c in enumerate(CHAR_VOCAB)}\n    \n    BATCH_SIZE = 32\n    LEARNING_RATE = 5e-4\n    MAX_GRAD_NORM = 1.0\n    EPOCHS = 8\n    DROPOUT_RATE = 0.1\n    PATIENCE = 2\n\nconfig = SFTConfig()\nos.makedirs(config.SAVE_DIR, exist_ok=True)\n\n# --- Dataset (Optimized) ---\nclass HangmanMaskedWordDataset(Dataset):\n    def __init__(self, words, tokenizer, mask_range):\n        self.words = [w for w in words if 0 < len(w) <= (config.MAX_LENGTH - 2)]\n        self.tokenizer = tokenizer\n        self.mask_range = mask_range\n\n    def __len__(self):\n        return len(self.words)\n\n    def __getitem__(self, idx):\n        word = self.words[idx]\n        word_len = len(word)\n        \n        mask_prob = random.uniform(*self.mask_range)\n        mask_indices = [i for i in range(word_len) if random.random() < mask_prob]\n        if not mask_indices and word_len > 0:\n            mask_indices = [random.randint(0, word_len - 1)]\n\n        masked_word = ''.join(\n            config.MASK_CHAR if i in mask_indices else ch for i, ch in enumerate(word)\n        )\n\n        inputs = self.tokenizer(\n            masked_word,\n            padding='max_length',\n            truncation=True,\n            max_length=config.MAX_LENGTH,\n            return_tensors=\"pt\"\n        )\n\n        labels = torch.full((config.MAX_LENGTH,), -100, dtype=torch.long)\n        for i in mask_indices:\n            if i < (config.MAX_LENGTH - 1) and word[i] in config.char2idx:\n                labels[i + 1] = config.char2idx[word[i]]\n\n        return {\n            \"input_ids\": inputs['input_ids'].squeeze(0),\n            \"attention_mask\": inputs['attention_mask'].squeeze(0),\n            \"labels\": labels\n        }\n\n# --- Model ---\nclass CanineForHangmanSFT(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.canine = CanineModel.from_pretrained(config.MODEL_ID)\n        self.config = self.canine.config\n        \n        self.dropout = nn.Dropout(config.DROPOUT_RATE)\n        self.cls_head = nn.Linear(self.canine.config.hidden_size, len(config.CHAR_VOCAB))\n\n    def forward(self, input_ids, attention_mask, **kwargs):\n        out = self.canine(input_ids=input_ids, attention_mask=attention_mask)\n        x = self.dropout(out.last_hidden_state)\n        return self.cls_head(x)\n\n# --- Training Function ---\ndef train_canine_lora(words, expert_name, mask_range, num_epochs):\n    print(f\"\\n=== Training CANINE LoRA Expert: {expert_name} ===\")\n    \n    tokenizer = CanineTokenizer.from_pretrained(config.MODEL_ID)\n\n    train_words, val_words = train_test_split(words, test_size=0.05, random_state=42)\n    train_dataset = HangmanMaskedWordDataset(train_words, tokenizer, mask_range)\n    val_dataset = HangmanMaskedWordDataset(val_words, tokenizer, mask_range)\n\n    train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n    base_model = CanineForHangmanSFT(config)\n\n    peft_config = LoraConfig(\n        task_type=TaskType.TOKEN_CLS,\n        r=32, \n        lora_alpha=64, \n        lora_dropout=0.1,\n        inference_mode=False,\n        target_modules=[\"query\", \"key\", \"value\", \"dense\"]\n    )\n\n    model = get_peft_model(base_model, peft_config).to(device)\n    model.print_trainable_parameters()\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=config.LEARNING_RATE)\n    scheduler = get_scheduler(\"linear\", optimizer, 0, num_epochs * len(train_loader))\n    loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n\n    best_val_loss = float('inf')\n    patience_counter = 0\n\n    for epoch in range(num_epochs):\n        model.train()\n        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n        for batch in progress_bar:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(batch['input_ids'], batch['attention_mask'])\n            loss = loss_fn(logits.view(-1, len(config.CHAR_VOCAB)), batch['labels'].view(-1))\n            \n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), config.MAX_GRAD_NORM)\n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n            progress_bar.set_postfix(loss=loss.item())\n\n        model.eval()\n        total_val_loss = 0\n        with torch.no_grad():\n            for batch in val_loader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                logits = model(batch['input_ids'], batch['attention_mask'])\n                loss = loss_fn(logits.view(-1, len(config.CHAR_VOCAB)), batch['labels'].view(-1))\n                total_val_loss += loss.item()\n        \n        avg_val_loss = total_val_loss / len(val_loader)\n        print(f\"Epoch {epoch+1} | Validation Loss: {avg_val_loss:.4f}\")\n\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            patience_counter = 0\n            save_path = os.path.join(config.SAVE_DIR, expert_name)\n            model.save_pretrained(save_path)\n            print(f\"✅ Validation loss improved. Saved expert to {save_path}\")\n        else:\n            patience_counter += 1\n            if patience_counter >= config.PATIENCE:\n                print(\"⛔ Early stopping triggered.\")\n                break\n\n# --- Main Execution ---\nif __name__ == '__main__':\n    with open(config.WORD_LIST_PATH, 'r') as f:\n        words = [line.strip().lower() for line in f if line.strip().isalpha()]\n\n    # Train your CANINE experts\n    train_canine_lora(words, expert_name=\"canine_early\", mask_range=(0.45, 0.8), num_epochs=config.EPOCHS)\n    train_canine_lora(words, expert_name=\"canine_late\", mask_range=(0.1, 0.4), num_epochs=config.EPOCHS)\n    \n    # --- CHANGE: Zip the final output directory for easy download ---\n    print(\"\\nZipping trained experts for download...\")\n    shutil.make_archive(\n        base_name=\"/kaggle/working/lora_experts\", # Name of the zip file\n        format='zip',                             # Format\n        root_dir=\"/kaggle/working/\",              # Root directory to zip\n        base_dir=\"lora_experts\"                   # The specific folder to zip\n    )\n    print(\"✅ Finished. You can now download lora_experts.zip from the output directory.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T16:07:25.082803Z","iopub.execute_input":"2025-07-21T16:07:25.083507Z","iopub.status.idle":"2025-07-21T18:53:24.210640Z","shell.execute_reply.started":"2025-07-21T16:07:25.083478Z","shell.execute_reply":"2025-07-21T18:53:24.209796Z"}},"outputs":[{"name":"stderr","text":"2025-07-21 16:07:30.177738: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753114050.199972     108 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753114050.207256     108 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"\n=== Training CANINE LoRA Expert: canine_early ===\nUsing device: cuda\ntrainable params: 6,242,304 || all params: 138,345,242 || trainable%: 4.5121\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1/8:   0%|          | 0/6748 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d929bf2fafe438ea9f2d4c2878bd40c"}},"metadata":{}},{"name":"stdout","text":"Epoch 1 | Validation Loss: 2.3886\n✅ Validation loss improved. Saved expert to /kaggle/working/lora_experts/canine_early\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2/8:   0%|          | 0/6748 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f8788ecf0d74e61ac6e854e43971c45"}},"metadata":{}},{"name":"stdout","text":"Epoch 2 | Validation Loss: 2.3458\n✅ Validation loss improved. Saved expert to /kaggle/working/lora_experts/canine_early\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3/8:   0%|          | 0/6748 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0585b5a3e54c4b208e15916cc9a63b4b"}},"metadata":{}},{"name":"stdout","text":"Epoch 3 | Validation Loss: 2.3295\n✅ Validation loss improved. Saved expert to /kaggle/working/lora_experts/canine_early\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 4/8:   0%|          | 0/6748 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68b914eb820e4d15bf1a3653fc9e7608"}},"metadata":{}},{"name":"stdout","text":"Epoch 4 | Validation Loss: 2.2963\n✅ Validation loss improved. Saved expert to /kaggle/working/lora_experts/canine_early\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 5/8:   0%|          | 0/6748 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9df36018c95a41d99ec68606a19746fc"}},"metadata":{}},{"name":"stdout","text":"Epoch 5 | Validation Loss: 2.2826\n✅ Validation loss improved. Saved expert to /kaggle/working/lora_experts/canine_early\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 6/8:   0%|          | 0/6748 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b437a116e5446ff9fbfa04d360c4769"}},"metadata":{}},{"name":"stdout","text":"Epoch 6 | Validation Loss: 2.2668\n✅ Validation loss improved. Saved expert to /kaggle/working/lora_experts/canine_early\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 7/8:   0%|          | 0/6748 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd0e0535d5b54699bfe1bf4d40fa5094"}},"metadata":{}},{"name":"stdout","text":"Epoch 7 | Validation Loss: 2.2646\n✅ Validation loss improved. Saved expert to /kaggle/working/lora_experts/canine_early\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 8/8:   0%|          | 0/6748 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09d375892aa045139850535236c1ba7d"}},"metadata":{}},{"name":"stdout","text":"Epoch 8 | Validation Loss: 2.2474\n✅ Validation loss improved. Saved expert to /kaggle/working/lora_experts/canine_early\n\n=== Training CANINE LoRA Expert: canine_late ===\nUsing device: cuda\ntrainable params: 6,242,304 || all params: 138,345,242 || trainable%: 4.5121\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1/8:   0%|          | 0/6748 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e451aeb037b4db3b684fda1a62b5374"}},"metadata":{}},{"name":"stdout","text":"Epoch 1 | Validation Loss: 1.9641\n✅ Validation loss improved. Saved expert to /kaggle/working/lora_experts/canine_late\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2/8:   0%|          | 0/6748 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4cae7384a7b149819511a3e1a85c8c6d"}},"metadata":{}},{"name":"stdout","text":"Epoch 2 | Validation Loss: 1.9183\n✅ Validation loss improved. Saved expert to /kaggle/working/lora_experts/canine_late\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3/8:   0%|          | 0/6748 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24979fc455fe4cba9abfe7dfc4ef81f3"}},"metadata":{}},{"name":"stdout","text":"Epoch 3 | Validation Loss: 1.8298\n✅ Validation loss improved. Saved expert to /kaggle/working/lora_experts/canine_late\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 4/8:   0%|          | 0/6748 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a4c135902df4c83850c67af0d8d33e6"}},"metadata":{}},{"name":"stdout","text":"Epoch 5 | Validation Loss: 1.7483\n✅ Validation loss improved. Saved expert to /kaggle/working/lora_experts/canine_late\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 6/8:   0%|          | 0/6748 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c95b28fb8db4c8f9527f8491fe617d2"}},"metadata":{}},{"name":"stdout","text":"Epoch 6 | Validation Loss: 1.7172\n✅ Validation loss improved. Saved expert to /kaggle/working/lora_experts/canine_late\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 7/8:   0%|          | 0/6748 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be29c7b3670b4d7098144c3be3180e01"}},"metadata":{}},{"name":"stdout","text":"Epoch 7 | Validation Loss: 1.6787\n✅ Validation loss improved. Saved expert to /kaggle/working/lora_experts/canine_late\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 8/8:   0%|          | 0/6748 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2e4305384554a5f8c57ada9dd6ae007"}},"metadata":{}},{"name":"stdout","text":"Epoch 8 | Validation Loss: 1.6741\n✅ Validation loss improved. Saved expert to /kaggle/working/lora_experts/canine_late\n\nZipping trained experts for download...\n✅ Finished. You can now download lora_experts.zip from the output directory.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}