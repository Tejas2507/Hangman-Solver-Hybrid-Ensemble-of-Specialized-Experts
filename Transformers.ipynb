{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training TRANSFORMER Expert: late ---\n",
      "Epoch 1/25 | Train Loss: 2.2672 | Val Loss: 2.0123\n",
      "Validation improved. Saved to transformer_late.pth\n",
      "Epoch 2/25 | Train Loss: 2.0798 | Val Loss: 1.9158\n",
      "Validation improved. Saved to transformer_late.pth\n",
      "Epoch 3/25 | Train Loss: 2.0300 | Val Loss: 1.8564\n",
      "Validation improved. Saved to transformer_late.pth\n",
      "Epoch 4/25 | Train Loss: 2.0003 | Val Loss: 1.8312\n",
      "Validation improved. Saved to transformer_late.pth\n",
      "Epoch 5/25 | Train Loss: 1.9809 | Val Loss: 1.8183\n",
      "Validation improved. Saved to transformer_late.pth\n",
      "Epoch 6/25 | Train Loss: 1.9714 | Val Loss: 1.7977\n",
      "Validation improved. Saved to transformer_late.pth\n",
      "Epoch 7/25 | Train Loss: 1.9549 | Val Loss: 1.7912\n",
      "Validation improved. Saved to transformer_late.pth\n",
      "Epoch 8/25 | Train Loss: 1.9455 | Val Loss: 1.7781\n",
      "Validation improved. Saved to transformer_late.pth\n",
      "Epoch 9/25 | Train Loss: 1.9373 | Val Loss: 1.7886\n",
      "No improvement. Patience 1/3\n",
      "Epoch 10/25 | Train Loss: 1.9297 | Val Loss: 1.7769\n",
      "Validation improved. Saved to transformer_late.pth\n",
      "Epoch 11/25 | Train Loss: 1.9227 | Val Loss: 1.7611\n",
      "Validation improved. Saved to transformer_late.pth\n",
      "Epoch 12/25 | Train Loss: 1.9159 | Val Loss: 1.7521\n",
      "Validation improved. Saved to transformer_late.pth\n",
      "Epoch 13/25 | Train Loss: 1.9138 | Val Loss: 1.7617\n",
      "No improvement. Patience 1/3\n",
      "Epoch 14/25 | Train Loss: 1.9099 | Val Loss: 1.7468\n",
      "Validation improved. Saved to transformer_late.pth\n",
      "Epoch 15/25 | Train Loss: 1.9052 | Val Loss: 1.7524\n",
      "No improvement. Patience 1/3\n",
      "Epoch 16/25 | Train Loss: 1.9005 | Val Loss: 1.7329\n",
      "Validation improved. Saved to transformer_late.pth\n",
      "Epoch 17/25 | Train Loss: 1.8971 | Val Loss: 1.7375\n",
      "No improvement. Patience 1/3\n",
      "Epoch 18/25 | Train Loss: 1.8918 | Val Loss: 1.7458\n",
      "No improvement. Patience 2/3\n",
      "Epoch 19/25 | Train Loss: 1.8115 | Val Loss: 1.6826\n",
      "Validation improved. Saved to transformer_late.pth\n",
      "Epoch 20/25 | Train Loss: 1.7809 | Val Loss: 1.6675\n",
      "Validation improved. Saved to transformer_late.pth\n",
      "Epoch 21/25 | Train Loss: 1.7695 | Val Loss: 1.6550\n",
      "Validation improved. Saved to transformer_late.pth\n",
      "Epoch 22/25 | Train Loss: 1.7600 | Val Loss: 1.6576\n",
      "No improvement. Patience 1/3\n",
      "Epoch 23/25 | Train Loss: 1.7535 | Val Loss: 1.6407\n",
      "Validation improved. Saved to transformer_late.pth\n",
      "Epoch 24/25 | Train Loss: 1.7457 | Val Loss: 1.6335\n",
      "Validation improved. Saved to transformer_late.pth\n",
      "Epoch 25/25 | Train Loss: 1.7404 | Val Loss: 1.6301\n",
      "Validation improved. Saved to transformer_late.pth\n",
      "✅ Training complete for expert: late\n",
      "\n",
      "--- Training TRANSFORMER Expert: mid ---\n",
      "Epoch 1/25 | Train Loss: 2.3444 | Val Loss: 2.1545\n",
      "Validation improved. Saved to transformer_mid.pth\n",
      "Epoch 2/25 | Train Loss: 2.1945 | Val Loss: 2.0908\n",
      "Validation improved. Saved to transformer_mid.pth\n",
      "Epoch 3/25 | Train Loss: 2.1543 | Val Loss: 2.0534\n",
      "Validation improved. Saved to transformer_mid.pth\n",
      "Epoch 4/25 | Train Loss: 2.1308 | Val Loss: 2.0335\n",
      "Validation improved. Saved to transformer_mid.pth\n",
      "Epoch 5/25 | Train Loss: 2.1130 | Val Loss: 2.0143\n",
      "Validation improved. Saved to transformer_mid.pth\n",
      "Epoch 6/25 | Train Loss: 2.1015 | Val Loss: 2.0086\n",
      "Validation improved. Saved to transformer_mid.pth\n",
      "Epoch 7/25 | Train Loss: 2.0913 | Val Loss: 1.9933\n",
      "Validation improved. Saved to transformer_mid.pth\n",
      "Epoch 8/25 | Train Loss: 2.0827 | Val Loss: 1.9810\n",
      "Validation improved. Saved to transformer_mid.pth\n",
      "Epoch 9/25 | Train Loss: 2.0726 | Val Loss: 1.9748\n",
      "Validation improved. Saved to transformer_mid.pth\n",
      "Epoch 10/25 | Train Loss: 2.0649 | Val Loss: 1.9596\n",
      "Validation improved. Saved to transformer_mid.pth\n",
      "Epoch 11/25 | Train Loss: 2.0602 | Val Loss: 1.9562\n",
      "Validation improved. Saved to transformer_mid.pth\n",
      "Epoch 12/25 | Train Loss: 2.0546 | Val Loss: 1.9512\n",
      "Validation improved. Saved to transformer_mid.pth\n",
      "Epoch 13/25 | Train Loss: 2.0510 | Val Loss: 1.9475\n",
      "Validation improved. Saved to transformer_mid.pth\n",
      "Epoch 14/25 | Train Loss: 2.0452 | Val Loss: 1.9459\n",
      "Validation improved. Saved to transformer_mid.pth\n",
      "Epoch 15/25 | Train Loss: 2.0424 | Val Loss: 1.9437\n",
      "Validation improved. Saved to transformer_mid.pth\n",
      "Epoch 16/25 | Train Loss: 2.0370 | Val Loss: 1.9443\n",
      "No improvement. Patience 1/3\n",
      "Epoch 17/25 | Train Loss: 2.0357 | Val Loss: 1.9467\n",
      "No improvement. Patience 2/3\n",
      "Epoch 18/25 | Train Loss: 1.9742 | Val Loss: 1.8963\n",
      "Validation improved. Saved to transformer_mid.pth\n",
      "Epoch 19/25 | Train Loss: 1.9513 | Val Loss: 1.8869\n",
      "Validation improved. Saved to transformer_mid.pth\n",
      "Epoch 20/25 | Train Loss: 1.9421 | Val Loss: 1.8722\n",
      "Validation improved. Saved to transformer_mid.pth\n",
      "Epoch 21/25 | Train Loss: 1.9344 | Val Loss: 1.8677\n",
      "Validation improved. Saved to transformer_mid.pth\n",
      "Epoch 22/25 | Train Loss: 1.9302 | Val Loss: 1.8694\n",
      "No improvement. Patience 1/3\n",
      "Epoch 23/25 | Train Loss: 1.9249 | Val Loss: 1.8637\n",
      "Validation improved. Saved to transformer_mid.pth\n",
      "Epoch 24/25 | Train Loss: 1.9213 | Val Loss: 1.8538\n",
      "Validation improved. Saved to transformer_mid.pth\n",
      "Epoch 25/25 | Train Loss: 1.9177 | Val Loss: 1.8572\n",
      "No improvement. Patience 1/3\n",
      "✅ Training complete for expert: mid\n",
      "\n",
      "--- Training TRANSFORMER Expert: early ---\n",
      "Epoch 1/25 | Train Loss: 2.4833 | Val Loss: 2.3178\n",
      "Validation improved. Saved to transformer_early.pth\n",
      "Epoch 2/25 | Train Loss: 2.3714 | Val Loss: 2.2709\n",
      "Validation improved. Saved to transformer_early.pth\n",
      "Epoch 3/25 | Train Loss: 2.3426 | Val Loss: 2.2438\n",
      "Validation improved. Saved to transformer_early.pth\n",
      "Epoch 4/25 | Train Loss: 2.3261 | Val Loss: 2.2381\n",
      "Validation improved. Saved to transformer_early.pth\n",
      "Epoch 5/25 | Train Loss: 2.3147 | Val Loss: 2.2082\n",
      "Validation improved. Saved to transformer_early.pth\n",
      "Epoch 6/25 | Train Loss: 2.3041 | Val Loss: 2.2027\n",
      "Validation improved. Saved to transformer_early.pth\n",
      "Epoch 7/25 | Train Loss: 2.2959 | Val Loss: 2.1909\n",
      "Validation improved. Saved to transformer_early.pth\n",
      "Epoch 8/25 | Train Loss: 2.2889 | Val Loss: 2.1893\n",
      "Validation improved. Saved to transformer_early.pth\n",
      "Epoch 9/25 | Train Loss: 2.2839 | Val Loss: 2.1865\n",
      "Validation improved. Saved to transformer_early.pth\n",
      "Epoch 10/25 | Train Loss: 2.2796 | Val Loss: 2.1742\n",
      "Validation improved. Saved to transformer_early.pth\n",
      "Epoch 11/25 | Train Loss: 2.2745 | Val Loss: 2.1745\n",
      "No improvement. Patience 1/3\n",
      "Epoch 12/25 | Train Loss: 2.2702 | Val Loss: 2.1719\n",
      "Validation improved. Saved to transformer_early.pth\n",
      "Epoch 13/25 | Train Loss: 2.2674 | Val Loss: 2.1682\n",
      "Validation improved. Saved to transformer_early.pth\n",
      "Epoch 14/25 | Train Loss: 2.2635 | Val Loss: 2.1660\n",
      "Validation improved. Saved to transformer_early.pth\n",
      "Epoch 15/25 | Train Loss: 2.2617 | Val Loss: 2.1609\n",
      "Validation improved. Saved to transformer_early.pth\n",
      "Epoch 16/25 | Train Loss: 2.2602 | Val Loss: 2.1555\n",
      "Validation improved. Saved to transformer_early.pth\n",
      "Epoch 17/25 | Train Loss: 2.2574 | Val Loss: 2.1607\n",
      "No improvement. Patience 1/3\n",
      "Epoch 18/25 | Train Loss: 2.2554 | Val Loss: 2.1525\n",
      "Validation improved. Saved to transformer_early.pth\n",
      "Epoch 19/25 | Train Loss: 2.2534 | Val Loss: 2.1544\n",
      "No improvement. Patience 1/3\n",
      "Epoch 20/25 | Train Loss: 2.2516 | Val Loss: 2.1530\n",
      "No improvement. Patience 2/3\n",
      "Epoch 21/25 | Train Loss: 2.2094 | Val Loss: 2.1183\n",
      "Validation improved. Saved to transformer_early.pth\n",
      "Epoch 22/25 | Train Loss: 2.1940 | Val Loss: 2.1093\n",
      "Validation improved. Saved to transformer_early.pth\n",
      "Epoch 23/25 | Train Loss: 2.1875 | Val Loss: 2.1044\n",
      "Validation improved. Saved to transformer_early.pth\n",
      "Epoch 24/25 | Train Loss: 2.1834 | Val Loss: 2.1090\n",
      "No improvement. Patience 1/3\n",
      "Epoch 25/25 | Train Loss: 2.1800 | Val Loss: 2.0999\n",
      "Validation improved. Saved to transformer_early.pth\n",
      "✅ Training complete for expert: early\n"
     ]
    }
   ],
   "source": [
    "# char_transformer_masked_train_expert.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "# --- Constants ---\n",
    "MASK_IDX = 26\n",
    "PAD_IDX = 27\n",
    "VOCAB_SIZE = 28\n",
    "CHAR_VOCAB = list(\"abcdefghijklmnopqrstuvwxyz\")\n",
    "char2idx = {ch: i for i, ch in enumerate(CHAR_VOCAB)}\n",
    "char2idx[\"_\"] = MASK_IDX\n",
    "\n",
    "# --- Dataset ---\n",
    "class MultiMaskCharDataset(Dataset):\n",
    "    def __init__(self, words, min_mask=1, max_mask=2):\n",
    "        self.data = []\n",
    "        for word in words:\n",
    "            if not word.isalpha() or len(word) < 4:\n",
    "                continue\n",
    "            word = word.lower()\n",
    "            n_mask = random.randint(min_mask, min(max_mask, len(word)))\n",
    "            mask_indices = random.sample(range(len(word)), n_mask)\n",
    "\n",
    "            input_seq = [char2idx.get(ch, MASK_IDX) for ch in word]\n",
    "            label_seq = [-100] * len(word)\n",
    "\n",
    "            for i in mask_indices:\n",
    "                input_seq[i] = MASK_IDX\n",
    "                label_seq[i] = char2idx[word[i]]\n",
    "\n",
    "            self.data.append((input_seq, label_seq))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx][0]), torch.tensor(self.data[idx][1])\n",
    "\n",
    "# --- Collate Function ---\n",
    "def collate_fn(batch):\n",
    "    inputs, labels = zip(*batch)\n",
    "    max_len = max(len(seq) for seq in inputs)\n",
    "    padded_inputs = torch.full((len(inputs), max_len), PAD_IDX, dtype=torch.long)\n",
    "    padded_labels = torch.full((len(labels), max_len), -100, dtype=torch.long)\n",
    "    for i in range(len(batch)):\n",
    "        padded_inputs[i, :len(inputs[i])] = inputs[i]\n",
    "        padded_labels[i, :len(labels[i])] = labels[i]\n",
    "    return padded_inputs, padded_labels\n",
    "\n",
    "# --- Transformer Model ---\n",
    "class CharTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size=VOCAB_SIZE, embed_dim=128, nhead=4, num_layers=4, ff_dim=256, dropout=0.2, max_len=20):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=PAD_IDX)\n",
    "        self.pos_embedding = nn.Embedding(max_len, embed_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=nhead, dim_feedforward=ff_dim, dropout=dropout, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.ln = nn.LayerNorm(embed_dim)\n",
    "        self.fc = nn.Linear(embed_dim, 26)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        positions = torch.arange(0, T, device=x.device).unsqueeze(0).expand(B, T)\n",
    "        emb = self.embedding(x) + self.pos_embedding(positions)\n",
    "        out = self.transformer(emb)\n",
    "        out = self.ln(out)\n",
    "        return self.fc(out)\n",
    "\n",
    "# --- Training Loop ---\n",
    "def train_transformer_expert(all_words, expert_name, min_mask, max_mask, epochs=25, batch_size=64):\n",
    "    print(f\"\\n--- Training TRANSFORMER Expert: {expert_name} ---\")\n",
    "    train_words, val_words = train_test_split(all_words, test_size=0.01, random_state=42)\n",
    "\n",
    "    train_dataset = MultiMaskCharDataset(train_words, min_mask=min_mask, max_mask=max_mask)\n",
    "    val_dataset = MultiMaskCharDataset(val_words, min_mask=min_mask, max_mask=max_mask)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    model = CharTransformer().to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.2, patience=1)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    patience_limit = 3\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            logits = model(inputs)\n",
    "            loss = criterion(logits.view(-1, 26), labels.view(-1))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                logits = model(inputs)\n",
    "                loss = criterion(logits.view(-1, 26), labels.view(-1))\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        scheduler.step(avg_val_loss)\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            save_name = f\"transformer_{expert_name}.pth\"\n",
    "            torch.save(model.state_dict(), save_name)\n",
    "            print(f\"Validation improved. Saved to {save_name}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"No improvement. Patience {patience_counter}/{patience_limit}\")\n",
    "\n",
    "        if patience_counter >= patience_limit:\n",
    "            print(\"Early stopping.\")\n",
    "            break\n",
    "\n",
    "    print(f\"✅ Training complete for expert: {expert_name}\")\n",
    "\n",
    "# --- Execute ---\n",
    "if __name__ == '__main__':\n",
    "    with open(\"words_250000_train.txt\") as f:\n",
    "        words = [line.strip().lower() for line in f if line.strip().isalpha()]\n",
    "\n",
    "    # Expert 1: Late-game (1–2 masks)\n",
    "    train_transformer_expert(words, expert_name=\"late\", min_mask=1, max_mask=2)\n",
    "\n",
    "    # Expert 2: Mid-game (2–4 masks)\n",
    "    train_transformer_expert(words, expert_name=\"mid\", min_mask=2, max_mask=4)\n",
    "\n",
    "    # Expert 3: Early-game (4–6 masks)\n",
    "    train_transformer_expert(words, expert_name=\"early\", min_mask=4, max_mask=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
